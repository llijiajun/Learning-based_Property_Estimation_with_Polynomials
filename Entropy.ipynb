{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_info=np.load(\"learned_ndv_estimator/model_training/training_data/rfs_F_infos.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s=np.load(\"learned_ndv_estimator/model_training/training_data/rfs_f_s.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial import Chebyshev\n",
    "from numpy.polynomial import Hermite\n",
    "from scipy import optimize\n",
    "from scipy.special import comb\n",
    "from estndv import ndvEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib import style\n",
    "#plt.rcParams['font.sans-serif']=['simhei']\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "from collections import Counter\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiajun_li/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:79: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "epoch 40\n",
      "epoch 41\n",
      "epoch 42\n",
      "epoch 43\n",
      "epoch 44\n",
      "epoch 45\n",
      "epoch 46\n",
      "epoch 47\n",
      "epoch 48\n",
      "epoch 49\n",
      "epoch 50\n",
      "epoch 51\n",
      "epoch 52\n",
      "epoch 53\n",
      "epoch 54\n",
      "epoch 55\n",
      "epoch 56\n",
      "epoch 57\n",
      "epoch 58\n",
      "epoch 59\n",
      "epoch 60\n",
      "epoch 61\n",
      "epoch 62\n",
      "epoch 63\n",
      "epoch 64\n",
      "epoch 65\n",
      "epoch 66\n",
      "epoch 67\n",
      "epoch 68\n",
      "epoch 69\n",
      "epoch 70\n",
      "epoch 71\n",
      "epoch 72\n",
      "epoch 73\n",
      "epoch 74\n",
      "epoch 75\n",
      "epoch 76\n",
      "epoch 77\n",
      "epoch 78\n",
      "epoch 79\n",
      "epoch 80\n",
      "epoch 81\n",
      "epoch 82\n",
      "epoch 83\n",
      "epoch 84\n",
      "epoch 85\n",
      "epoch 86\n",
      "epoch 87\n",
      "epoch 88\n",
      "epoch 89\n",
      "epoch 90\n",
      "epoch 91\n",
      "epoch 92\n",
      "epoch 93\n",
      "epoch 94\n",
      "epoch 95\n",
      "epoch 96\n",
      "epoch 97\n",
      "epoch 98\n",
      "epoch 99\n",
      "epoch 100\n",
      "epoch 101\n",
      "epoch 102\n",
      "epoch 103\n",
      "epoch 104\n",
      "epoch 105\n",
      "epoch 106\n",
      "epoch 107\n",
      "epoch 108\n",
      "epoch 109\n",
      "epoch 110\n",
      "epoch 111\n",
      "epoch 112\n",
      "epoch 113\n",
      "epoch 114\n",
      "epoch 115\n",
      "epoch 116\n",
      "epoch 117\n",
      "epoch 118\n",
      "epoch 119\n",
      "epoch 120\n",
      "epoch 121\n",
      "epoch 122\n",
      "epoch 123\n",
      "epoch 124\n",
      "epoch 125\n",
      "epoch 126\n",
      "epoch 127\n",
      "epoch 128\n",
      "epoch 129\n",
      "epoch 130\n",
      "epoch 131\n",
      "epoch 132\n",
      "epoch 133\n",
      "epoch 134\n",
      "epoch 135\n",
      "epoch 136\n",
      "epoch 137\n",
      "epoch 138\n",
      "epoch 139\n",
      "epoch 140\n",
      "epoch 141\n",
      "epoch 142\n",
      "epoch 143\n",
      "epoch 144\n",
      "epoch 145\n",
      "epoch 146\n",
      "epoch 147\n",
      "epoch 148\n",
      "epoch 149\n",
      "epoch 150\n",
      "epoch 151\n",
      "epoch 152\n",
      "epoch 153\n",
      "epoch 154\n",
      "epoch 155\n",
      "epoch 156\n",
      "epoch 157\n",
      "epoch 158\n",
      "epoch 159\n",
      "epoch 160\n",
      "epoch 161\n",
      "epoch 162\n",
      "epoch 163\n",
      "epoch 164\n",
      "epoch 165\n",
      "epoch 166\n",
      "epoch 167\n",
      "epoch 168\n",
      "epoch 169\n",
      "epoch 170\n",
      "epoch 171\n",
      "epoch 172\n",
      "epoch 173\n",
      "epoch 174\n",
      "epoch 175\n",
      "epoch 176\n",
      "epoch 177\n",
      "epoch 178\n",
      "epoch 179\n",
      "epoch 180\n",
      "epoch 181\n",
      "epoch 182\n",
      "epoch 183\n",
      "epoch 184\n",
      "epoch 185\n",
      "epoch 186\n",
      "epoch 187\n",
      "epoch 188\n",
      "epoch 189\n",
      "epoch 190\n",
      "epoch 191\n",
      "epoch 192\n",
      "epoch 193\n",
      "epoch 194\n",
      "epoch 195\n",
      "epoch 196\n",
      "epoch 197\n",
      "epoch 198\n",
      "epoch 199\n",
      "epoch 200\n",
      "epoch 201\n",
      "epoch 202\n",
      "epoch 203\n",
      "epoch 204\n",
      "epoch 205\n",
      "epoch 206\n",
      "epoch 207\n",
      "epoch 208\n",
      "epoch 209\n",
      "epoch 210\n",
      "epoch 211\n",
      "epoch 212\n",
      "epoch 213\n",
      "epoch 214\n",
      "epoch 215\n",
      "epoch 216\n",
      "epoch 217\n",
      "epoch 218\n",
      "epoch 219\n",
      "epoch 220\n",
      "epoch 221\n",
      "epoch 222\n",
      "epoch 223\n",
      "epoch 224\n",
      "epoch 225\n",
      "epoch 226\n",
      "epoch 227\n",
      "epoch 228\n",
      "epoch 229\n",
      "epoch 230\n",
      "epoch 231\n",
      "epoch 232\n",
      "epoch 233\n",
      "epoch 234\n",
      "epoch 235\n",
      "epoch 236\n",
      "epoch 237\n",
      "epoch 238\n",
      "epoch 239\n",
      "epoch 240\n",
      "epoch 241\n",
      "epoch 242\n",
      "epoch 243\n",
      "epoch 244\n",
      "epoch 245\n",
      "epoch 246\n",
      "epoch 247\n",
      "epoch 248\n",
      "epoch 249\n",
      "epoch 250\n",
      "epoch 251\n",
      "epoch 252\n",
      "epoch 253\n",
      "epoch 254\n",
      "epoch 255\n",
      "epoch 256\n",
      "epoch 257\n",
      "epoch 258\n",
      "epoch 259\n",
      "epoch 260\n",
      "epoch 261\n",
      "epoch 262\n",
      "epoch 263\n",
      "epoch 264\n",
      "epoch 265\n",
      "epoch 266\n",
      "epoch 267\n",
      "epoch 268\n",
      "epoch 269\n",
      "epoch 270\n",
      "epoch 271\n",
      "epoch 272\n",
      "epoch 273\n",
      "epoch 274\n",
      "epoch 275\n",
      "epoch 276\n",
      "epoch 277\n",
      "epoch 278\n",
      "epoch 279\n",
      "epoch 280\n",
      "epoch 281\n",
      "epoch 282\n",
      "epoch 283\n",
      "epoch 284\n",
      "epoch 285\n",
      "epoch 286\n",
      "epoch 287\n",
      "epoch 288\n",
      "epoch 289\n",
      "epoch 290\n",
      "epoch 291\n",
      "epoch 292\n",
      "epoch 293\n",
      "epoch 294\n",
      "epoch 295\n",
      "epoch 296\n",
      "epoch 297\n",
      "epoch 298\n",
      "epoch 299\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from scipy.special import comb\n",
    "from scipy import interpolate\n",
    "import scipy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PolyEntropyNet(nn.Module):\n",
    "    def __init__(self,L=7,H=200):\n",
    "        super(PolyEntropyNet,self).__init__()\n",
    "        self.L = L\n",
    "        self.H = H\n",
    "        self.weight = Parameter(torch.Tensor(self.H))\n",
    "        self.weight.requires_grad = True\n",
    "        self.fc1 = nn.Linear(self.L*10, self.L*10//2)\n",
    "        self.fc2 = nn.Linear(self.L*10//2, self.L)\n",
    "        self.fc3 = nn.Linear(self.L, self.L)\n",
    "        self.fc1.requires_grad = True\n",
    "        self.fc2.requires_grad = True\n",
    "        self.fc3.requires_grad = True\n",
    "        self.i = torch.Tensor( [ i+1 for i in range(self.H) ] ).float().to(device)\n",
    "    def set_paras(self):\n",
    "        self.weight.data = torch.Tensor([1.0 for i in range(self.H)])\n",
    "    def forward(self,fi,M_tensor,N,n):\n",
    "        bt_temp= F.leaky_relu(self.fc1( torch.log(fi+1) ) )\n",
    "        bt=F.leaky_relu(self.fc2(bt_temp))\n",
    "        temp = torch.einsum('ij,ijk->ik', bt,M_tensor)\n",
    "        pi=(self.i.reshape(self.H,1)/N.reshape(1,N.shape[0])).t()\n",
    "        temp=(( temp+pi*torch.log(pi)).abs()*(self.weight/self.weight.sum()).reshape(self.weight.shape[0],1)).sum(1)\n",
    "        f0 = F.relu(self.fc3( bt ).sum(1) )\n",
    "        return f0,temp\n",
    "    \n",
    "penet = PolyEntropyNet().to(device)\n",
    "penet.set_paras()\n",
    "for param in penet.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(penet.parameters(),lr=0.001)\n",
    "\n",
    "# data prepare\n",
    "fea = torch.tensor([[0.0 for j in range(70)]  for i in range(f_s.shape[0])]).to(device)\n",
    "entropy_plus=[]\n",
    "for i in range(f_s.shape[0]):\n",
    "    f_s_n=sum(f_s[i][:,0]*f_s[i][:,1])\n",
    "    entropy_p=0.0\n",
    "    for item in f_s[i]:\n",
    "        if item[0]>70:\n",
    "            entropy_p+= item[1]*item[0]/f_s_n*np.log(f_s_n/item[0])#+1.0/f_s_n\n",
    "        else:\n",
    "            fea[i][item[0]-1]=item[1]\n",
    "            entropy_p+= item[1]*item[0]/f_s_n*np.log(f_s_n/item[0])#+1.0/f_s_n\n",
    "    entropy_plus.append(entropy_p)\n",
    "entropy_plus = torch.Tensor(entropy_plus).to(device)\n",
    "\n",
    "d = torch.tensor([ sum(f[:,1]) for f in f_s]).to(device)\n",
    "n = torch.tensor([ sum(f[:,0]*f[:,1]) for f in f_s] ).to(device)\n",
    "N = torch.tensor(F_info[:,0]).to(device)\n",
    "Entro = torch.tensor( F_info[:,2]).to(device)\n",
    "\n",
    "M_tensor=[]\n",
    "size = F_info.shape[0]\n",
    "L=7\n",
    "H=200\n",
    "data_n=np.array([sum(te[:,1]) for te in f_s])\n",
    "data_N=F_info[:,0]\n",
    "for s in range(size):\n",
    "    M=[]\n",
    "    for t in range(1,L+1):\n",
    "        M_temp=[]\n",
    "        for j in range(1,H+1):\n",
    "            M_temp.append( comb(data_n[s],t)*(j/(data_N[s]-j))**t)\n",
    "        M.append(M_temp)\n",
    "    M=np.array(M)\n",
    "    M[M<0.001]=0\n",
    "    M_tensor.append(M)    \n",
    "M_tensor = torch.tensor(M_tensor).float().to(device)\n",
    "\n",
    "def lossfun_entropy(y,y_pred,l0_loss):\n",
    "    l=torch.mean(  torch.square(y_pred-y) + l0_loss.abs() )\n",
    "    return l\n",
    "\n",
    "batch_size=200\n",
    "train_loss=[]\n",
    "for i in range(300):\n",
    "    print(\"epoch\",i)\n",
    "    for epoch in range(200):\n",
    "        predict,l0_loss=penet(\n",
    "            fea[epoch*batch_size:((epoch+1)*batch_size)] , \n",
    "            M_tensor[epoch*batch_size:((epoch+1)*batch_size)],\n",
    "            N[epoch*batch_size:((epoch+1)*batch_size)],\n",
    "            n[epoch*batch_size:((epoch+1)*batch_size)])\n",
    "        y=torch.relu(Entro-entropy_plus)\n",
    "        loss=lossfun_entropy(y[epoch*batch_size:((epoch+1)*batch_size)],predict,\n",
    "                             l0_loss)\n",
    "        loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1494325524199114\n",
      "0.9729270963998351\n",
      "19.06044869781375\n",
      "0.45239484097276417\n"
     ]
    }
   ],
   "source": [
    "def Fi2fi(Fi,q):\n",
    "    fs = []\n",
    "    for item in Fi:\n",
    "        f_i=np.random.binomial(item[0],q,size=item[1])\n",
    "        fs.append(f_i)\n",
    "    fs = np.concatenate(fs)\n",
    "    fs = fs[fs>0]\n",
    "    unique, counts = np.unique(fs,return_counts=True)\n",
    "    f_sparse = np.array([unique,counts]).T\n",
    "    return f_sparse\n",
    "renEsti=ndvEstimator()\n",
    "import os\n",
    "Dir=\"kasandr\"\n",
    "q=0.001\n",
    "kasResult=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        r5.append( np.abs(temp-En) )   \n",
    "    kasResult.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "kasResult=np.array(kasResult)\n",
    "for i in range(kasResult.shape[1]):\n",
    "    print(np.mean(kasResult[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6522908480608957\n",
      "0.5056012315676519\n",
      "3.7746788642610976\n",
      "0.28172469564846586\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"kasandr\"\n",
    "q=0.005\n",
    "kasResult=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    kasResult.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "kasResult=np.array(kasResult)\n",
    "for i in range(kasResult.shape[1]):\n",
    "    print(np.mean(kasResult[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4753002368141702\n",
      "0.3466162879613011\n",
      "1.8904896427993687\n",
      "0.21500141492911748\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"kasandr\"\n",
    "q=0.01\n",
    "kasResult=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    kasResult.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "kasResult=np.array(kasResult)\n",
    "for i in range(kasResult.shape[1]):\n",
    "    print(np.mean(kasResult[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02762446863182404\n",
      "0.010060569487000381\n",
      "20.500976415872174\n",
      "0.027624452114105226\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"Airlines\"\n",
    "q=0.001\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006507923906911151\n",
      "0.0024870071460573184\n",
      "4.090577266367129\n",
      "0.006507933139801025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"Airlines\"\n",
    "q=0.005\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        \n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003653386469935449\n",
      "0.00176315590363747\n",
      "2.043636135477109\n",
      "0.0036533832550048827\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"Airlines\"\n",
    "q=0.01\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        #print(temp)\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5018011285061659\n",
      "1.2928135443675135\n",
      "17.260647106108873\n",
      "0.14898082789252787\n"
     ]
    }
   ],
   "source": [
    "def SVD_entropy(fi,q,N=10**6):\n",
    "    M=[]\n",
    "    for j in range(10,200):\n",
    "        M_temp=[]\n",
    "        for t in range(1,10):\n",
    "            M_temp.append( comb(N*q,t)*(j/(N-j))**t )\n",
    "        M.append(M_temp)\n",
    "    M=np.array(M)\n",
    "    M[M<0.01]=0\n",
    "    u,s,v = np.linalg.svd(M,full_matrices=False)\n",
    "    s_inv=( np.array( [ (i+1)/N*np.log(N/(i+1)) for i in range(s.shape[0]) ] )  /s)\n",
    "    s_inv[np.isinf(s_inv)] = 0\n",
    "    inv = np.matmul(v.T*s_inv , u.T )\n",
    "    bi=[]\n",
    "    for i in range(9):\n",
    "        #print(sum(inv[i,:]))\n",
    "        bi.append(sum(inv[i,:]))\n",
    "    result=0\n",
    "    for x in fi:\n",
    "        if x[0]<10:\n",
    "            result+=x[1]*bi[x[0]-1]\n",
    "    if result>0:\n",
    "        return result+sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )#sum(fi[:,1])\n",
    "    else:\n",
    "        return sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )#sum(fi[:,1])\n",
    "def Fi2fi(Fi,q):\n",
    "    fs = []\n",
    "    for item in Fi:\n",
    "        f_i=np.random.binomial(item[0],q,size=item[1])\n",
    "        fs.append(f_i)\n",
    "    fs = np.concatenate(fs)\n",
    "    fs = fs[fs>0]\n",
    "    unique, counts = np.unique(fs,return_counts=True)\n",
    "    f_sparse = np.array([unique,counts]).T\n",
    "    return f_sparse\n",
    "renEsti=ndvEstimator()\n",
    "import os\n",
    "Dir=\"SSB\"\n",
    "q=0.001\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9015412751562009\n",
      "0.7231101312219057\n",
      "3.3651043361863358\n",
      "0.0532607541364782\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"SSB\"\n",
    "q=0.005\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        #print(temp)\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6792526787289997\n",
      "0.5176179112817797\n",
      "1.6784082447383017\n",
      "0.036526027847738826\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"SSB\"\n",
    "q=0.01\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5250490632133745\n",
      "0.45866272453639106\n",
      "21.707687820522725\n",
      "0.2721954326753662\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"NCVoter\"\n",
    "q=0.001\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        \n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36006606383667095\n",
      "0.31423977609826736\n",
      "4.1941368200135605\n",
      "0.2046379915784559\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"NCVoter\"\n",
    "q=0.005\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "\n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29943270476430606\n",
      "0.2594498012301002\n",
      "2.068844398940328\n",
      "0.18806978335364682\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Dir=\"NCVoter\"\n",
    "q=0.01\n",
    "Result=[]\n",
    "order_D = []\n",
    "En_list=[]\n",
    "for filename in os.listdir(Dir):\n",
    "    pathname=os.path.join(Dir,filename)\n",
    "    Fi=np.loadtxt(pathname,delimiter=\",\",dtype=int)\n",
    "    if len(Fi.shape)==1:\n",
    "        Fi=np.array([Fi])\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    r3=[]\n",
    "    r4=[]\n",
    "    r5=[]\n",
    "    r6=[]\n",
    "    r7=[]\n",
    "    order_D.append(sum(Fi[:,1]))\n",
    "    En_list.append( sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) ) )\n",
    "    for r in range(5):\n",
    "        fii=Fi2fi(Fi,q)\n",
    "        fi_temp=fii\n",
    "        max_i=max(max(fi_temp[:,0]),L)\n",
    "        fi_dict={ item[0]:item[1] for item in fi_temp }\n",
    "        fi=[]\n",
    "        n=0\n",
    "        for i in range(1,max_i+1):\n",
    "            if i in fi_dict.keys():\n",
    "                fi.append([i,fi_dict[i]])\n",
    "                n+=i*fi_dict[i]\n",
    "            else:\n",
    "                fi.append([i,0])\n",
    "        fi=np.array(fi)\n",
    "        \n",
    "        N=sum(Fi[:,0]*Fi[:,1])\n",
    "        En=sum( Fi[:,1]*Fi[:,0]/sum(Fi[:,1]*Fi[:,0])*np.log( sum(Fi[:,1]*Fi[:,0])/Fi[:,0] ) )\n",
    "        from entropy import *\n",
    "        entro_estimator = Entropy(N)\n",
    "        temp=entro_estimator.estimate_plug( fii )*np.log(2)\n",
    "        r1.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate_Miller_Madow( fii )*np.log(2)\n",
    "        r2.append( np.abs(temp-En) )\n",
    "        temp=entro_estimator.estimate(fii)*np.log(2)\n",
    "        r3.append( np.abs(temp-En) )\n",
    "        \n",
    "        \n",
    "        L=7\n",
    "        H=200\n",
    "        data_n=n\n",
    "        data_N=N\n",
    "        M=[]\n",
    "        for t in range(1,L+1):\n",
    "            M_temp=[]\n",
    "            for j in range(1,H+1):\n",
    "                M_temp.append( comb(n,t)*(j/(N-j))**t)\n",
    "            M.append(M_temp)\n",
    "        M=np.array(M)\n",
    "        M[M<0.001]=0\n",
    "        feat=[0.0 for i in range(70)]\n",
    "        for item in fii:\n",
    "            if item[0]>70:\n",
    "                continue\n",
    "            else:\n",
    "                feat[item[0]-1]=item[1]\n",
    "        temp=penet(torch.tensor([feat]).float().to(device),\n",
    "                  torch.tensor([M]).float().to(device),\n",
    "                  torch.tensor([N]).float().to(device),\n",
    "                  torch.tensor([n]).float().to(device),\n",
    "                 )[0].cpu().detach().numpy()+ sum( fi[:,1]*fi[:,0]/sum(fi[:,1]*fi[:,0])*np.log( sum(fi[:,1]*fi[:,0])/fi[:,0] ) )\n",
    "        \n",
    "        r5.append( np.abs(temp-En) )\n",
    "        \n",
    "    Result.append([np.median(r1),\n",
    "                  np.median(r2), \n",
    "                  np.median(r3),\n",
    "                  np.median(r5)])\n",
    "        \n",
    "\n",
    "Result=np.array(Result)\n",
    "for i in range(Result.shape[1]):\n",
    "    print(np.mean(Result[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
